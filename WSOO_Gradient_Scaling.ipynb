{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauthamPrabhuM/ACMWinterSchool/blob/main/WSOO_Gradient_Scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Scaling gradients.}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "When we tried to solve certain problems of the form $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$ using gradient descent algorithm, we noticed that the algorithm needed a large number of iterations to find the minimizer (e.g. Rosenbrock's function). We will discuss some remedy measures for this issue.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Meohokl4xP"
      },
      "source": [
        "Consider the problem $\\min_{\\mathbf{x}} f(\\mathbf{x}) = 1500x_1^2 + 4x_1 x_2 +  x_2^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvWjvAgnXxS3"
      },
      "source": [
        "Note that the function $f(\\mathbf{x})$ is twice continuously differentiable. First let us investigate the Hessian $\\nabla^2 f(\\mathbf{x})$ of the function, given by:\n",
        "\\begin{align}\n",
        "\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix}\n",
        "                          -4000(x_2-3x_1^2)+2 & -4000 x_1 \\\\\n",
        "                          -4000 x_1 & 2000\n",
        "                          \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "We shall find the condition number of the Hessian given by $\\kappa\\left (\\nabla^2 f(\\mathbf{x}) \\right ) = \\frac{\\lambda_{\\max} \\left (\\nabla^2 f(\\mathbf{x}) \\right )}{\\lambda_{\\min} \\left (\\nabla^2 f(\\mathbf{x}) \\right )}$, where $\\lambda_{\\max}(A)$ denotes the maximum eigen value of matrix $A$ and $\\lambda_{\\min}(A)$ denotes the minimum eigen value of $A$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpL8QOYLOlre"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/1.19/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! \n",
        "import scipy as sp\n",
        "from scipy.linalg import sqrtm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g65QxvbDOp9Q"
      },
      "source": [
        "def evalh(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  hess = np.array([[1500., 2.],[2., 1.0]])\n",
        "  hess_sqrt = sqrtm(hess)\n",
        "  print(hess_sqrt)\n",
        "  return hess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExOTGqQPbcfL"
      },
      "source": [
        "def find_condition_number_point(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  \n",
        "  eig_vals = np.linalg.eigvalsh(evalh(x))\n",
        "  print(eig_vals)\n",
        "  cond_number = eig_vals[-1]/eig_vals[0]\n",
        "  print(cond_number)\n",
        "\n",
        "  cond_check = np.linalg.cond(evalh(x))\n",
        "  print(cond_check)\n",
        "  return cond_check\n",
        "  \n",
        "\n",
        "def find_condition_number_matrix(A):\n",
        "  assert type(A) is np.ndarray \n",
        "  assert A.shape[0] == A.shape[1]\n",
        "\n",
        "  #print(A)\n",
        "  eig_vals = np.linalg.eigvalsh(A)\n",
        "  #print(eig_vals)\n",
        "  cond_number = eig_vals[-1]/eig_vals[0]\n",
        "  #print(cond_number)\n",
        "\n",
        "  cond_check = np.linalg.cond(A)\n",
        "  print(cond_check)\n",
        "  return cond_check\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPZfgdw7cG8H",
        "outputId": "80381b69-aa14-4818-dbf2-688c439b729a"
      },
      "source": [
        "#A = np.array([[32002, -8000], [-8000, 2000]])\n",
        "A = np.array([[1500., 2.], [2., 1.]])\n",
        "find_condition_number_matrix(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1504.016046343423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1504.016046343423"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6DIfamCfON0",
        "outputId": "092fe020-e5d7-46db-9d02-dfb9086df646"
      },
      "source": [
        "x = np.array([5.,5.])\n",
        "find_condition_number_point(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "[9.97331559e-01 1.50000267e+03]\n",
            "1504.016046343423\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1504.016046343423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1504.016046343423"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "#Is the problem well-conditioned? \n",
        "\n",
        "The condition number of the Hessian plays a major role in the progress of the iterates of gradient descent towards the optimal solution point. \n",
        "\n",
        "Typically a large value of the condition number indicates that the problem is $\\textbf{ill-conditioned}$ and hence leads to slow progress of the iterates towards the optimal solution point. \n",
        "\n",
        "Now we shall discuss a method which would help in better $\\textbf{conditioning}$ of the problem and hence would help in speeding up the progress of the iterates towards the optimal solution point. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Induced Conditioning\n",
        "\n",
        "Let us first illustrate an equivalent transformation of the problem $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. \n",
        "\n",
        "Consider the transformation $\\mathbf{x}=\\mathbf{My}$ where $\\mathbf{M}\\in {\\mathbb{R}}^{n \\times n}$ is an invertible matrix and $\\mathbf{y} \\in {\\mathbb{R}}^n$.\n",
        "\n",
        "Now consider the equivalent problem $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y}) \\equiv \\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} f(\\mathbf{My})$. \n",
        "\n",
        "Note that the gradient $\\nabla g(\\mathbf{y}) = \\mathbf{M}^\\top \\nabla f(\\mathbf{My})$ and the Hessian is $\\nabla^2 g(\\mathbf{y}) = \\mathbf{M}^\\top \\nabla^2 f(\\mathbf{My}) \\mathbf{M}$. \n",
        "\n",
        "Hence the gradient descent update to solve $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y})$ becomes: \n",
        "\\begin{align}\n",
        "{\\mathbf{y}}^{k+1} &= {\\mathbf{y}}^{k} - \\eta \\nabla g(\\mathbf{y}^{k}) \\\\\n",
        "&= {\\mathbf{y}}^{k} - \\eta {\\mathbf{M}}^\\top \\nabla f(\\mathbf{M}\\mathbf{y}^{k}) \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Pre-multiplying by $\\mathbf{M}$, we have:\n",
        "\\begin{align}\n",
        "{\\mathbf{M}\\mathbf{y}}^{k+1} &= {\\mathbf{M}\\mathbf{y}}^{k} -  \\eta \\mathbf{MM}^\\top \\nabla f(\\mathbf{M}\\mathbf{y}^{k})  \\\\\n",
        "\\implies \\mathbf{x}^{k+1} &= \\mathbf{x}^{k} - \\eta \\mathbf{MM}^\\top \\nabla f({\\mathbf{x}}^{k}) \n",
        "\\end{align}\n",
        "\n",
        "Letting $\\mathbf{D} = \\mathbf{MM}^\\top$, we see that the update is of the form:\n",
        "\\begin{align}\n",
        "\\mathbf{x}^{k+1} &= \\mathbf{x}^{k} - \\eta \\mathbf{D} \\nabla f({\\mathbf{x}}^{k}) \n",
        "\\end{align}\n",
        "\n",
        "Note that the matrix $\\mathbf{D}$ is symmetric and positive definite and hence can be written as $\\mathbf{D} = \\mathbf{B}^2$, where $\\mathbf{B}$ is also symmetric and positive definite. \n",
        "\n",
        "Denoting $\\mathbf{B}= \\mathbf{D}^{\\frac{1}{2}}$, we see that a useful choice for the matrix $\\mathbf{M}$ is $\\mathbf{M} = \\mathbf{B} = \\mathbf{D}^{\\frac{1}{2}}$. \n",
        "\n",
        "The matrix $\\mathbf{D}$ is called a $\\textbf{scaling}$ matrix and helps in scaling the Hessian. \n",
        "\n",
        "We will consider $\\mathbf{D}$ to be a diagonal matrix. Thus it would be useful to find a useful candidate of the scaling matrix at each iteration which could help in significant progress of the iterates towards the optimal solution. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "70eFm-tBwO90"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgHGXcTYNXuw"
      },
      "source": [
        "This discussion leads to the following algorithm:\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\text{ Choose a suitable scaling matrix }\\mathbf{D}^k. \\\\ \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{D}^k \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\mathbf{D}^k \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{D}^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k \\mathbf{D}^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoM2VgZqfNVJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  #hess = evalh(x)\n",
        "  #print(hess[0][1])\n",
        "  return np.array([[np.sqrt(1./1500.), 0.],[0., 1.]]), np.array([[1./1500., 0.],[0., 1.]])\n",
        "  \n",
        "  #np.array([[np.sqrt(1./(20000.+2.*(x[1]**2))), 0.],[0., np.sqrt(1./(2.*(x[0]**2)))]]), np.array([[1./(20000.+2.*(x[1]**2)), 0.],[0., 1./(2.*(x[0]**2))]])\n",
        "  #return np.array([[np.sqrt(1./(-4000.*(x[1]-3.*x[0]**2)+2.)), 0.],[0., np.sqrt(1./2000)]]), np.array([[1./(-4000.*(x[1]-3.*x[0]**2)+2.), 0.],[0., 1./2000]])\n",
        "  #hess_inv = np.linalg.inv(evalh(x))\n",
        "  #hess_inv_sqrt = sqrtm(hess_inv)\n",
        "  #return hess_inv_sqrt, hess_inv\n",
        "  #return np.array([[np.sqrt(1./32002), 0], [0, np.sqrt(1./32002)]]), np.array([[1./32002, 0], [0, np.sqrt(1./32002)]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOCq_3eyd0lb",
        "outputId": "f79bda49-1a73-40f9-d872-1a3ce6c94474"
      },
      "source": [
        "x = np.array([5., 5.])\n",
        "D_k_sqrt,D_k = compute_D_k(x)\n",
        "print(D_k)\n",
        "hess=evalh(x)\n",
        "find_condition_number_matrix(hess)\n",
        "print(hess)\n",
        "DHessD=np.matmul(D_k_sqrt, np.matmul( hess, D_k_sqrt))\n",
        "print(DHessD)\n",
        "find_condition_number_matrix(DHessD)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6.66666667e-04 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00]]\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1504.016046343423\n",
            "[[1.5e+03 2.0e+00]\n",
            " [2.0e+00 1.0e+00]]\n",
            "[[1.         0.05163978]\n",
            " [0.05163978 1.        ]]\n",
            "1.1089032980269364\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1089032980269364"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return (1500.0*(x[0]**2)+ 4.0*x[0]*x[1] + (x[1]**2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyorbuF04H5A",
        "outputId": "d2d57110-fdd2-4ea1-a044-a1520743f3c6"
      },
      "source": [
        "my_x = np.array([-0.5921619, 110.92350273])\n",
        "evalf(my_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12567.268343084725"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([3000.0*x[0]+4.0*x[1], 4.0*x[0]+2.0*x[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array\n",
        "   \n",
        "  #Complete the code \n",
        "  step_length = 0.\n",
        "  return step_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using simple backtracking\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  \n",
        "  #Complete the code\n",
        "  alpha = 0\n",
        "  return alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqjdHM3eaHYf"
      },
      "source": [
        "def compute_steplength_backtracking_dirn(x, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  gradf_dot_direction = np.dot(gradf, direction)\n",
        "  evalf_x = evalf(x)\n",
        "  while evalf(np.add(x,np.multiply(-alpha, direction)))>evalf_x  - alpha * gamma * gradf_dot_direction:\n",
        "    alpha = rho * alpha\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLRu5s635ph"
      },
      "source": [
        "#line search type \n",
        "CONSTANT_STEP_LENGTH = 3\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "EXACT_LINE_SEARCH = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdYW5nldqZU-"
      },
      "source": [
        "def find_minimizer_gd(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  A = np.array([[1500., 2.],[2.,1.]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "    if args is None:\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    elif len(args)<3 :\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    else:\n",
        "      alpha_start = float(args[0])\n",
        "      rho = float(args[1])\n",
        "      gamma = float(args[2])\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    #step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "    elif line_search_type == EXACT_LINE_SEARCH:\n",
        "      raise ValueError('Exact search not yet implemented')\n",
        "      #step_length= compute_steplength_exact(g_x, A)\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH:  \n",
        "      step_length = 0.1\n",
        "    else:\n",
        "      err_msg = 'line search type:'+str(line_search_type)+' unknown! please check ...'\n",
        "      raise ValueError(err_msg)\n",
        "\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    if k%100 == 0:\n",
        "      print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  \n",
        "  print('alpha_start:',alpha_start, 'rho:', rho, 'iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))      \n",
        "  return x, evalf(x),k \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer_gdscaling(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "    if args is None:\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    elif len(args)<3 :\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    else:\n",
        "      alpha_start = float(args[0])\n",
        "      rho = float(args[1])\n",
        "      gamma = float(args[2])\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    D_k_sqrt,D_k = compute_D_k(x)\n",
        "    \n",
        "\n",
        "    #step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      #step_length = compute_steplength_backtracking_dirn() #call the new function with appropriate arguments to compute the steplength \n",
        "    elif line_search_type == EXACT_LINE_SEARCH:\n",
        "      raise ValueError('Exact search not yet implemented')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH:  \n",
        "      step_length = 0.1\n",
        "    else:\n",
        "      err_msg = 'line search type:'+str(line_search_type)+' unknown! please check ...'\n",
        "      raise ValueError(err_msg)\n",
        "    #Scaled Hessian \n",
        "    scaled_Hessian = np.matmul(D_k_sqrt, np.matmul(evalh(x),D_k_sqrt))\n",
        "    \n",
        "    #print('condition number of scaled Hessian:',find_condition_number_matrix(scaled_Hessian) )\n",
        "    x = np.subtract(x, np.multiply(step_length,D_k_mul_g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    #if 0: # k%10000 == 0:\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x), 'Hessian cond num:', find_condition_number_matrix(scaled_Hessian))\n",
        "  print('alpha_start:',alpha_start, 'rho:', rho, 'iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))      \n",
        "  return x, evalf(x),k \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0_iOLVoQFYy",
        "outputId": "f93d4657-f44e-407b-8082-c36a56ba89ea"
      },
      "source": [
        "my_start_x = np.array([1.,4000.])\n",
        "my_tol= 1e-12\n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "#x_opt, opt_fval, num_iters = find_minimizer_gd(my_start_x, my_tol, 1)\n",
        "x_opt, opt_fval, num_iters = find_minimizer_gd(my_start_x, my_tol, 2,alpha_start, rho, gamma)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 100  x: [-2.54036972e+00  3.39229410e+03]  f(x): 11482868.773155304  grad at x: [5948.06725698 6774.42672692]  gradient norm: 9015.118500166325\n",
            "iter: 200  x: [1.22341526e-01 2.86570279e+03]  f(x): 8213677.304012541  grad at x: [11829.83573425  5731.89494416]  gradient norm: 13145.327426513373\n",
            "iter: 300  x: [  -5.03012151 2426.7621057 ]  f(x): 5878299.868176542  grad at x: [-5383.31610584  4833.40372535]  gradient norm: 7234.7690956696015\n",
            "iter: 400  x: [  -4.15485624 2054.0587983 ]  f(x): 4210914.516376751  grad at x: [-4248.33351619  4091.49817166]  gradient norm: 5898.194211237012\n",
            "iter: 500  x: [  -3.43568202 1738.59543872]  f(x): 3016526.9216023567  grad at x: [-3352.66430783  3463.44814936]  gradient norm: 4820.35590431472\n",
            "iter: 600  x: [  -2.84405663 1471.58109072]  f(x): 2160942.8538846574  grad at x: [-2645.84551263  2931.78595493]  gradient norm: 3949.1603363544345\n",
            "iter: 700  x: [  -3.40856946 1243.14251685]  f(x): 1545881.4853983072  grad at x: [-5253.13832493  2472.65075584]  gradient norm: 5805.985189542082\n",
            "iter: 800  x: [-6.09715844e-01  1.05273596e+03]  f(x): 1106243.1489691364  grad at x: [2381.79630154 2103.03305335]  gradient norm: 3177.3733878019607\n",
            "iter: 900  x: [-5.61638245e-01  8.91056136e+02]  f(x): 792452.3895811102  grad at x: [1879.30980928 1779.86571971]  gradient norm: 2588.3831515955962\n",
            "iter: 1000  x: [-5.11335120e-01  7.54207202e+02]  f(x): 567678.0887089162  grad at x: [1482.82344938 1506.36906388]  gradient norm: 2113.7438677997347\n",
            "iter: 1100  x: [-4.61174889e-01  6.38375612e+02]  f(x): 406664.83406984137  grad at x: [1169.97778093 1274.90652423]  gradient norm: 1730.3856949824963\n",
            "iter: 1200  x: [5.65695437e-02 5.39279556e+02]  f(x): 290949.2672339744  grad at x: [2326.82685644 1078.78539083]  gradient norm: 2564.741924500036\n",
            "iter: 1300  x: [3.39508930e-03 4.56456649e+02]  f(x): 208358.88883978265  grad at x: [1836.01186517  912.92687899]  gradient norm: 2050.4572795889294\n",
            "iter: 1400  x: [ -0.7939017  386.54105504]  f(x): 149131.90469102503  grad at x: [-835.54087158  769.90650329]  gradient norm: 1136.1710134915866\n",
            "iter: 1500  x: [ -0.65602869 327.17589079]  f(x): 106831.0768880084  grad at x: [-659.38249527  651.72766683]  gradient norm: 927.1106874472168\n",
            "iter: 1600  x: [ -0.54269315 276.92805627]  f(x): 76529.77429631146  grad at x: [-520.36723827  551.68533992]  gradient norm: 758.3790456899609\n",
            "iter: 1700  x: [ -0.44941739 234.39730797]  f(x): 54823.69306303095  grad at x: [-410.66292872  466.99694639]  gradient norm: 621.8763453964251\n",
            "iter: 1800  x: [ -0.53578931 198.01101636]  f(x): 39214.59913158368  grad at x: [-815.32385641  393.87887549]  gradient norm: 905.479739915347\n",
            "iter: 1900  x: [-1.00358753e-01  1.67682546e+02]  f(x): 28065.23024740055  grad at x: [369.65392281 334.96365603]  gradient norm: 498.8433356393868\n",
            "iter: 2000  x: [-9.20171642e-02  1.41929760e+02]  f(x): 20104.5174791255  grad at x: [291.66754561 283.49145038]  gradient norm: 406.73991640780076\n",
            "iter: 2100  x: [-8.34652733e-02  1.20132102e+02]  f(x): 14402.064267615267  grad at x: [230.13258952 239.93034369]  gradient norm: 332.45688229278574\n",
            "iter: 2200  x: [-7.50499200e-02  1.01682143e+02]  f(x): 10317.18196831305  grad at x: [181.57881157 203.06408607]  gradient norm: 272.40757673168304\n",
            "iter: 2300  x: [5.84551711e-03 8.58978606e+01]  f(x): 7380.502186978031  grad at x: [361.12799389 171.81910334]  gradient norm: 399.91903211349654\n",
            "iter: 2400  x: [-0.15176459 72.74085195]  f(x): 5281.62233736483  grad at x: [-164.33037141  144.87464554]  gradient norm: 219.0733527504795\n",
            "iter: 2500  x: [-0.12532032 61.56927652]  f(x): 3783.4700590268376  grad at x: [-129.68384392  122.63727177]  gradient norm: 178.4875340238318\n",
            "iter: 2600  x: [-0.1035988  52.11343684]  f(x): 2710.3138075478955  grad at x: [-102.34263839  103.8124785 ]  gradient norm: 145.77738619058408\n",
            "iter: 2700  x: [-0.08573521 44.10982946]  f(x): 1941.5757824393695  grad at x: [-80.76630866  87.87671809]  gradient norm: 119.35457341863822\n",
            "iter: 2800  x: [-0.10313609 37.26250879]  f(x): 1389.0777029106875  grad at x: [-160.35823508   74.11247321]  gradient norm: 176.65622617219447\n",
            "iter: 2900  x: [-0.0842343 31.5397165]  f(x): 994.7699388586892  grad at x: [-126.54402732   62.74249581]  gradient norm: 141.2445100916311\n",
            "iter: 3000  x: [-1.64885184e-02  2.67089158e+01]  f(x): 712.012429298639  grad at x: [57.3701082  53.35187756]  gradient norm: 78.34380738012865\n",
            "iter: 3100  x: [-1.50537407e-02  2.26069445e+01]  f(x): 510.0525874335855  grad at x: [45.26655612 45.1536741 ]  gradient norm: 63.936807769532344\n",
            "iter: 3200  x: [-1.36078438e-02  1.91349565e+01]  f(x): 365.38277976877146  grad at x: [35.71629462 38.2154817 ]  gradient norm: 52.30752090313617\n",
            "iter: 3300  x: [2.12410091e-03 1.61646073e+01]  f(x): 261.43863814584455  grad at x: [71.03073195 32.33771102]  gradient norm: 78.04545108683403\n",
            "iter: 3400  x: [4.39862873e-04 1.36820363e+01]  f(x): 187.22248071785626  grad at x: [56.04773386 27.36583207]  gradient norm: 62.37176634026806\n",
            "iter: 3500  x: [-0.02395034 11.58635435]  f(x): 133.99404663324268  grad at x: [-25.50559188  23.07690735]  gradient norm: 34.395913565277326\n",
            "iter: 3600  x: [-0.01978528  9.80691639]  f(x): 95.98666474194721  grad at x: [-20.12817592  19.53469166]  gradient norm: 28.049022161091912\n",
            "iter: 3700  x: [-0.01636255  8.30076535]  f(x): 68.76101824576105  grad at x: [-15.88459694  16.53608049]  gradient norm: 22.929508891274335\n",
            "iter: 3800  x: [-0.0135465  7.0259297]  f(x): 49.25824249982296  grad at x: [-12.53576802  13.99767341]  gradient norm: 18.790432157718854\n",
            "iter: 3900  x: [-0.01620993  5.93527061]  f(x): 35.23673872020086  grad at x: [-24.88871365  11.80570149]  gradient norm: 27.546735829020335\n",
            "iter: 4000  x: [-2.94009022e-03  5.02619174e+00]  f(x): 25.216459817555446  grad at x: [11.28449631 10.04062313]  gradient norm: 15.104766457040295\n",
            "iter: 4100  x: [-2.70441981e-03  4.25426619e+00]  f(x): 18.063730389663785  grad at x: [8.90380533 8.49771471]  gradient norm: 12.308082897808044\n",
            "iter: 4200  x: [-2.45941630e-03  3.60089345e+00]  f(x): 12.94008234609681  grad at x: [7.02532489 7.19194923]  gradient norm: 10.053821344305598\n",
            "iter: 4300  x: [-2.21611491e-03  3.04786610e+00]  f(x): 9.269836797798826  grad at x: [5.54311964 6.08686773]  gradient norm: 8.232626196812232\n",
            "iter: 4400  x: [2.41715052e-04 2.57474100e+00]  f(x): 6.631868274711035  grad at x: [11.02410916  5.15044886]  gradient norm: 12.167912976570214\n",
            "iter: 4500  x: [-6.1780097e-06  2.1793106e+00]  f(x): 4.7493408752072765  grad at x: [8.69870835 4.35859648]  gradient norm: 9.729588393407901\n",
            "iter: 4600  x: [-0.00378024  1.84550501]  f(x): 3.3994182776667334  grad at x: [-3.95870672  3.67588906]  gradient norm: 5.402177270923971\n",
            "iter: 4700  x: [-0.00312412  1.56207145]  f(x): 2.4351870245740224  grad at x: [-3.1240886  3.1116464]  gradient norm: 4.409339284694279\n",
            "iter: 4800  x: [-0.00258471  1.32216775]  f(x): 1.7444789505650902  grad at x: [-2.46544998  2.63399667]  gradient norm: 3.60782234024045\n",
            "iter: 4900  x: [-0.00312094  1.11692307]  f(x): 1.24818414705508  grad at x: [-4.89512502  2.22136239]  gradient norm: 5.375565071533601\n",
            "iter: 5000  x: [-0.00254815  0.94538554]  f(x): 0.8938574878917462  grad at x: [-3.86290315  1.8805785 ]  gradient norm: 4.296346846456631\n",
            "iter: 5100  x: [-4.83663572e-04  8.00585008e-01]  f(x): 0.6397383962436173  grad at x: [1.75134932 1.59923536]  gradient norm: 2.3716614815514614\n",
            "iter: 5200  x: [-4.42886277e-04  6.77630683e-01]  f(x): 0.45827711107720565  grad at x: [1.3818639  1.35348982]  gradient norm: 1.9342912217877704\n",
            "iter: 5300  x: [-4.01305512e-04  5.73559758e-01]  f(x): 0.32829167490112765  grad at x: [1.0903225  1.14551429]  gradient norm: 1.5814569693897982\n",
            "iter: 5400  x: [-3.60534924e-04  4.85472111e-01]  f(x): 0.23517802996733378  grad at x: [0.86028367 0.96950208]  gradient norm: 1.2961567344540297\n",
            "iter: 5500  x: [2.35055717e-05 4.10111493e-01]  f(x): 0.16823082470556086  grad at x: [1.71096269 0.82031701]  gradient norm: 1.8974491569197933\n",
            "iter: 5600  x: [-0.00072259  0.34729456]  f(x): 0.12039290899270211  grad at x: [-0.77858008  0.69169878]  gradient norm: 1.0414577031212648\n",
            "iter: 5700  x: [-0.00059675  0.2939569 ]  f(x): 0.0862431483865185  grad at x: [-0.61442887  0.58552678]  gradient norm: 0.8487428628771102\n",
            "iter: 5800  x: [-0.00049338  0.24881085]  f(x): 0.061780940996253864  grad at x: [-0.48488936  0.49564819]  gradient norm: 0.6933864873968899\n",
            "iter: 5900  x: [-0.00040835  0.21059836]  f(x): 0.04425780226875803  grad at x: [-0.38266312  0.41956331]  gradient norm: 0.5678595177059689\n",
            "iter: 6000  x: [-0.00049046  0.17790645]  f(x): 0.03166250861455978  grad at x: [-0.75975723  0.35385106]  gradient norm: 0.8381179069572707\n",
            "iter: 6100  x: [-0.00040063  0.1505835 ]  f(x): 0.022674833045938928  grad at x: [-0.59954996  0.29956449]  gradient norm: 0.6702231258841722\n",
            "iter: 6200  x: [-7.94228770e-05  1.27519281e-01]  f(x): 0.016230117163503272  grad at x: [0.27180849 0.25472087]  gradient norm: 0.3725084937877403\n",
            "iter: 6300  x: [-7.24250408e-05  1.07934793e-01]  f(x): 0.011626518959029778  grad at x: [0.21446405 0.21557989]  gradient norm: 0.3040880078783799\n",
            "iter: 6400  x: [-6.54052509e-05  9.13581036e-02]  f(x): 0.008328818656810715  grad at x: [0.16921666 0.18245459]  gradient norm: 0.24884524221141543\n",
            "iter: 6500  x: [9.27525568e-06 7.71764416e-02]  f(x): 0.0059591955084055325  grad at x: [0.33653153 0.15438998]  gradient norm: 0.37025631692728506\n",
            "iter: 6600  x: [1.41667092e-06 6.53236331e-02]  f(x): 0.0042675502236411466  grad at x: [0.26554455 0.13065293]  gradient norm: 0.2959461005105344\n",
            "iter: 6700  x: [-0.00011404  0.05531799]  f(x): 0.003054353870668558  grad at x: [-0.12084285  0.11017983]  gradient norm: 0.16353161688710502\n",
            "iter: 6800  x: [-9.42180624e-05  4.68222279e-02]  f(x): 0.00218799058947431  grad at x: [-0.09536528  0.09326758]  gradient norm: 0.13339182108313105\n",
            "iter: 6900  x: [-7.79282225e-05  3.96312471e-02]  f(x): 0.0015673913855585044  grad at x: [-0.07525968  0.07895078]  gradient norm: 0.10907449379328554\n",
            "iter: 7000  x: [-6.45239803e-05  3.35446605e-02]  f(x): 0.001122831524034995  grad at x: [-0.0593933   0.06683123]  gradient norm: 0.08940904099432082\n",
            "iter: 7100  x: [-7.70897522e-05  2.83374084e-02]  f(x): 0.00080318486472568  grad at x: [-0.11791962  0.05636646]  gradient norm: 0.13069894808798282\n",
            "iter: 7200  x: [-1.41748629e-05  2.39970937e-02]  f(x): 0.0005748012749168135  grad at x: [0.05346379 0.04793749]  gradient norm: 0.07180793255795971\n",
            "iter: 7300  x: [-1.30206411e-05  2.03116056e-02]  f(x): 0.0004117577489649975  grad at x: [0.0421845  0.04057113]  gradient norm: 0.05852818525971438\n",
            "iter: 7400  x: [-1.18279827e-05  1.71921371e-02]  f(x): 0.00029496603737461953  grad at x: [0.0332846  0.03433696]  gradient norm: 0.047821455424003415\n",
            "iter: 7500  x: [-1.06482831e-05  1.45517585e-02]  f(x): 0.00021130394935495135  grad at x: [0.02626218 0.02906092]  gradient norm: 0.03916937125911608\n",
            "iter: 7600  x: [1.01962851e-06 1.22928658e-02]  f(x): 0.00015116624598525966  grad at x: [0.05223035 0.02458981]  gradient norm: 0.05772926551101565\n",
            "iter: 7700  x: [-1.35565264e-07  1.04049195e-02]  f(x): 0.00010825673437800014  grad at x: [0.04121298 0.0208093 ]  gradient norm: 0.04616856849984015\n",
            "iter: 7800  x: [-1.80002427e-05  8.81119537e-03]  f(x): 7.748876232700895e-05  grad at x: [-0.01875595  0.01755039]  gradient norm: 0.02568660579813635\n",
            "iter: 7900  x: [-1.48778309e-05  7.45796767e-03]  f(x): 5.5509472995993874e-05  grad at x: [-0.01480162  0.01485642]  gradient norm: 0.020971441168444953\n",
            "iter: 8000  x: [-1.23104478e-05  6.31256931e-03]  f(x): 3.976500980378321e-05  grad at x: [-0.01168107  0.0125759 ]  gradient norm: 0.017163929823317332\n",
            "iter: 8100  x: [-1.48410256e-05  5.33264739e-03]  f(x): 2.845094437672738e-05  grad at x: [-0.02319249  0.01060593]  gradient norm: 0.025502494604504673\n",
            "iter: 8200  x: [-1.21188614e-05  4.51365709e-03]  f(x): 2.0374598990005635e-05  grad at x: [-0.01830196  0.00897884]  gradient norm: 0.02038580725246226\n",
            "iter: 8300  x: [-2.33057465e-06  3.82232005e-03]  f(x): 1.4582645124642473e-05  grad at x: [0.00829756 0.00763532]  gradient norm: 0.01127597080703751\n",
            "iter: 8400  x: [-2.13138082e-06  3.23528585e-03]  f(x): 1.044630617414139e-05  grad at x: [0.006547   0.00646205]  gradient norm: 0.009198981560683836\n",
            "iter: 8500  x: [-1.92930130e-06  2.73840872e-03]  f(x): 7.483332750913833e-06  grad at x: [0.00516573 0.0054691 ]  gradient norm: 0.007523020266291784\n",
            "iter: 8600  x: [-1.73183985e-06  2.31784229e-03]  f(x): 5.360835237903877e-06  grad at x: [0.00407585 0.00462876]  gradient norm: 0.006167490845503599\n",
            "iter: 8700  x: [9.13629454e-08 1.95803988e-03]  f(x): 3.834648243356101e-06  grad at x: [0.00810625 0.00391645]  gradient norm: 0.009002766526631829\n",
            "iter: 8800  x: [-3.44044532e-06  1.65812622e-03]  f(x): 2.74431878599195e-06  grad at x: [-0.00368883  0.00330249]  gradient norm: 0.0049511533398974095\n",
            "iter: 8900  x: [-2.84166109e-06  1.40347039e-03]  f(x): 1.9658889341677913e-06  grad at x: [-0.0029111   0.00279557]  gradient norm: 0.004036055998552461\n",
            "iter: 9000  x: [-2.34968572e-06  1.18792472e-03]  f(x): 1.4082816646356e-06  grad at x: [-0.00229736  0.00236645]  gradient norm: 0.0032981728300475943\n",
            "iter: 9100  x: [-1.94498413e-06  1.00548265e-03]  f(x): 1.0088472125277269e-06  grad at x: [-0.00181302  0.00200319]  gradient norm: 0.002701814124196149\n",
            "iter: 9200  x: [-2.33240909e-06  8.49398131e-04]  f(x): 7.217128073079837e-07  grad at x: [-0.00359963  0.00168947]  gradient norm: 0.003976388793034489\n",
            "iter: 9300  x: [-4.15033220e-07  7.19299638e-04]  f(x): 5.164562156002769e-07  grad at x: [0.0016321  0.00143694]  gradient norm: 0.002174520843813499\n",
            "iter: 9400  x: [-3.82513542e-07  6.08829167e-04]  f(x): 3.699608877753331e-07  grad at x: [0.00128778 0.00121613]  gradient norm: 0.0017712524175578846\n",
            "iter: 9500  x: [-3.48403605e-07  5.15324819e-04]  f(x): 2.650235829399086e-07  grad at x: [0.00101609 0.00102926]  gradient norm: 0.0014463069276121306\n",
            "iter: 9600  x: [-3.14336358e-07  4.36180927e-04]  f(x): 1.8985358225360354e-07  grad at x: [0.00080171 0.0008711 ]  gradient norm: 0.0011838789733365975\n",
            "iter: 9700  x: [4.01805490e-08 3.68471873e-04]  f(x): 1.3583316486481575e-07  grad at x: [0.00159443 0.0007371 ]  gradient norm: 0.0017565669026006346\n",
            "iter: 9800  x: [3.52585046e-09 3.11881722e-04]  f(x): 9.727462576331241e-08  grad at x: [0.0012581  0.00062378]  gradient norm: 0.001404252544637789\n",
            "iter: 9900  x: [-5.42994591e-07  2.64110708e-04]  f(x): 6.96230881351833e-08  grad at x: [-0.00057254  0.00052605]  gradient norm: 0.0007775160065940824\n",
            "iter: 10000  x: [-4.48674986e-07  2.23548457e-04]  f(x): 4.987467387945364e-08  grad at x: [-0.00045183  0.0004453 ]  gradient norm: 0.0006343858703165806\n",
            "iter: 10100  x: [-3.71145396e-07  1.89215774e-04]  f(x): 3.5728326407490616e-08  grad at x: [-0.00035657  0.00037695]  gradient norm: 0.0005188770414964616\n",
            "iter: 10200  x: [-3.07341207e-07  1.60155922e-04]  f(x): 2.5594717205457616e-08  grad at x: [-0.0002814   0.00031908]  gradient norm: 0.0004254404196538127\n",
            "iter: 10300  x: [-3.66621996e-07  1.35294373e-04]  f(x): 1.830777729170236e-08  grad at x: [-0.00055869  0.00026912]  gradient norm: 0.0006201287164545492\n",
            "iter: 10400  x: [-6.83288517e-08  1.14571934e-04]  f(x): 1.3102416947065564e-08  grad at x: [0.0002533  0.00022887]  gradient norm: 0.0003413842657666666\n",
            "iter: 10500  x: [-6.26805614e-08  9.69759073e-05]  f(x): 9.385905850469568e-09  grad at x: [0.00019986 0.0001937 ]  gradient norm: 0.00027832518770033217\n",
            "iter: 10600  x: [-5.68777719e-08  8.20822896e-05]  f(x): 6.723680249902759e-09  grad at x: [0.0001577  0.00016394]  gradient norm: 0.00022747162687086902\n",
            "iter: 10700  x: [-5.11597462e-08  6.94760428e-05]  f(x): 4.816629002297719e-09  grad at x: [0.00012442 0.00013875]  gradient norm: 0.0001863663539406208\n",
            "iter: 10800  x: [4.23127091e-09 5.86911654e-05]  f(x): 3.445673103318041e-09  grad at x: [0.00024746 0.0001174 ]  gradient norm: 0.0002738946545260068\n",
            "iter: 10900  x: [-1.03803475e-07  4.97014164e-05]  f(x): 2.4657568129752368e-09  grad at x: [-1.12604761e-04  9.89876188e-05]  gradient norm: 0.00014992791871365564\n",
            "iter: 11000  x: [-8.57122514e-08  4.20682487e-05]  f(x): 1.7663343744952374e-09  grad at x: [-8.88637596e-05  8.37936483e-05]  gradient norm: 0.0001221398512904919\n",
            "iter: 11100  x: [-7.08527224e-08  3.56073864e-05]  f(x): 1.265324608992012e-09  grad at x: [-7.01286216e-05  7.09313619e-05]  gradient norm: 9.974608602316203e-05\n",
            "iter: 11200  x: [-5.86329747e-08  3.01387864e-05]  f(x): 9.064346790488111e-10  grad at x: [-5.53437784e-05  6.00430410e-05]  gradient norm: 8.165843846661432e-05\n",
            "iter: 11300  x: [-7.05746832e-08  2.54602388e-05]  f(x): 6.485075443038143e-10  grad at x: [-1.09883094e-04  5.06381788e-05]  gradient norm: 0.00012098974999887047\n",
            "iter: 11400  x: [-5.76375259e-08  2.15500443e-05]  f(x): 4.644191689838476e-10  grad at x: [-8.67124006e-05  4.28695384e-05]  gradient norm: 9.673074865208927e-05\n",
            "iter: 11500  x: [-1.12283500e-08  1.82493182e-05]  f(x): 3.3240708966327587e-10  grad at x: [3.93122228e-05 3.64537230e-05]  gradient norm: 5.361272969831422e-05\n",
            "iter: 11600  x: [-1.02559668e-08  1.54465770e-05]  f(x): 2.381208393703612e-10  grad at x: [3.10184076e-05 3.08521301e-05]  gradient norm: 4.374923477828773e-05\n",
            "iter: 11700  x: [-9.27430891e-09  1.30742825e-05]  f(x): 1.7058086215673146e-10  grad at x: [2.44742032e-05 2.61114677e-05]  gradient norm: 3.5788201569387636e-05\n",
            "iter: 11800  x: [-8.31825283e-09  1.10663264e-05]  f(x): 1.221991610712254e-10  grad at x: [1.93105473e-05 2.20993799e-05]  gradient norm: 2.934756938794044e-05\n",
            "iter: 11900  x: [3.37361733e-10 9.34848260e-06]  f(x): 8.740691300132435e-11  grad at x: [3.84060156e-05 1.86983147e-05]  gradient norm: 4.271591046117412e-05\n",
            "iter: 12000  x: [-1.63811958e-08  7.91657241e-06]  f(x): 6.255590240529197e-11  grad at x: [-1.74772977e-05  1.57676200e-05]  gradient norm: 2.3538771736784294e-05\n",
            "iter: 12100  x: [-1.35318239e-08  6.70074136e-06]  f(x): 4.481190712317187e-11  grad at x: [-1.37925062e-05  1.33473554e-05]  gradient norm: 1.9193361425019918e-05\n",
            "iter: 12200  x: [-1.11904041e-08  5.67163821e-06]  f(x): 3.2101445959351335e-11  grad at x: [-1.08846595e-05  1.12985148e-05]  gradient norm: 1.5688602542513337e-05\n",
            "iter: 12300  x: [-9.26408925e-09  4.80058520e-06]  f(x): 2.2996461103038745e-11  grad at x: [-8.58992695e-06  9.56411405e-06]  gradient norm: 1.2855314948347426e-05\n",
            "iter: 12400  x: [-1.10920385e-08  4.05537394e-06]  f(x): 1.6450678287464006e-11  grad at x: [-1.70546199e-05  8.06637972e-06]  gradient norm: 1.8866015513597102e-05\n",
            "iter: 12500  x: [-2.00144954e-09  3.43423047e-06]  f(x): 1.1772453857554212e-11  grad at x: [7.73257327e-06 6.86045514e-06]  gradient norm: 1.0337240158620483e-05\n",
            "iter: 12600  x: [-1.84198796e-09  2.90679929e-06]  f(x): 8.433154321199496e-12  grad at x: [6.10123326e-06 5.80623062e-06]  gradient norm: 8.422432030196252e-06\n",
            "iter: 12700  x: [-1.67581950e-09  2.46037132e-06]  f(x): 6.041147015781883e-12  grad at x: [4.81402678e-06 4.91403935e-06]  gradient norm: 6.879145045923163e-06\n",
            "iter: 12800  x: [-1.51055371e-09  2.08250603e-06]  f(x): 4.3276710656294105e-12  grad at x: [3.79836298e-06 4.15896984e-06]  gradient norm: 5.632458738066013e-06\n",
            "iter: 12900  x: [1.72397475e-10 1.75923532e-06]  f(x): 3.096166626200212e-12  grad at x: [7.55413368e-06 3.51916022e-06]  gradient norm: 8.333632123545714e-06\n",
            "iter: 13000  x: [1.49308594e-12 1.48905081e-06]  f(x): 2.2172811981832516e-12  grad at x: [5.96068248e-06 2.97810758e-06]  gradient norm: 6.663246987037162e-06\n",
            "iter: 13100  x: [-2.58550999e-09  1.26097249e-06]  f(x): 1.5870378818983257e-12  grad at x: [-2.71264002e-06  2.51160294e-06]  gradient norm: 3.6968317802988108e-06\n",
            "iter: 13200  x: [-2.13665924e-09  1.06731172e-06]  f(x): 1.1368803441166948e-12  grad at x: [-2.14073085e-06  2.12607680e-06]  gradient norm: 3.0171064123870617e-06\n",
            "iter: 13300  x: [-1.76766094e-09  9.03393457e-07]  f(x): 8.14419102943793e-13  grad at x: [-1.68940899e-06  1.79971627e-06]  gradient norm: 2.4684167754746327e-06\n",
            "iter: 13400  x: [-2.13564598e-09  7.63156563e-07]  f(x): 5.827300862781227e-13  grad at x: [-3.35431169e-06  1.51777054e-06]  gradient norm: 3.681716220305401e-06\n",
            "iter: 13500  x: [-1.74359975e-09  6.45950648e-07]  f(x): 4.1730733262870247e-13  grad at x: [-2.64699665e-06  1.28492690e-06]  gradient norm: 2.9423848178168585e-06\n",
            "iter: 13600  x: [-3.29320189e-10  5.47013240e-07]  f(x): 2.9866559193551936e-13  grad at x: [1.20009239e-06 1.09270920e-06]  gradient norm: 1.6230326974867613e-06\n",
            "iter: 13700  x: [-3.01701187e-10  4.63002618e-07]  f(x): 2.1394920593386023e-13  grad at x: [9.46906910e-07 9.24798431e-07]  gradient norm: 1.3235878643218945e-06\n",
            "iter: 13800  x: [-2.734820e-10  3.918944e-07]  f(x): 1.53264704998102e-13  grad at x: [7.47131601e-07 7.82694872e-07]  gradient norm: 1.0820429249081318e-06\n",
            "iter: 13900  x: [-2.45775975e-10  3.31707026e-07]  f(x): 1.0979405714257098e-13  grad at x: [5.89500176e-07 6.62430947e-07]  gradient norm: 8.867498057213783e-07\n",
            "iter: 14000  x: [1.71845045e-11 2.80215610e-07]  f(x): 7.854049264646345e-14  grad at x: [1.17241595e-06 5.60499958e-07]  gradient norm: 1.299507358059945e-06\n",
            "iter: 14100  x: [-4.94229687e-10  2.37294880e-07]  f(x): 5.620614167643496e-14  grad at x: [-5.33509543e-07  4.72612841e-07]  gradient norm: 7.127379106336443e-07\n",
            "iter: 14200  x: [-4.08143838e-10  2.00851016e-07]  f(x): 4.0263098498599883e-14  grad at x: [-4.21027447e-07  4.00069458e-07]  gradient norm: 5.807922883971499e-07\n",
            "iter: 14300  x: [-3.37426460e-10  1.70004219e-07]  f(x): 2.884276365790346e-14  grad at x: [-3.32262505e-07  3.38658732e-07]  gradient norm: 4.7443451473929375e-07\n",
            "iter: 14400  x: [-2.79264373e-10  1.43894887e-07]  f(x): 2.066198249186699e-14  grad at x: [-2.62213572e-07  2.86672716e-07]  gradient norm: 3.885063749206351e-07\n",
            "iter: 14500  x: [-3.35614229e-10  1.21557589e-07]  f(x): 1.478201695245051e-14  grad at x: [-5.20612332e-07  2.41772721e-07]  gradient norm: 5.740132830699433e-07\n",
            "iter: 14600  x: [-2.74129172e-10  1.02888722e-07]  f(x): 1.0585990016789789e-14  grad at x: [-4.10832630e-07  2.04680926e-07]  gradient norm: 4.5899643950742024e-07\n",
            "iter: 14700  x: [-5.40883641e-11  8.71297041e-08]  f(x): 7.577122845805114e-15  grad at x: [1.86253724e-07 1.74043055e-07]  gradient norm: 2.5491456335467194e-07\n",
            "iter: 14800  x: [-4.93446538e-11  7.37482720e-08]  f(x): 5.4279036282011556e-15  grad at x: [1.46959127e-07 1.47299165e-07]  gradient norm: 2.0807217250786386e-07\n",
            "iter: 14900  x: [-4.45779965e-11  6.24219685e-08]  f(x): 3.888352360649845e-15  grad at x: [1.15953884e-07 1.24665625e-07]  gradient norm: 1.7025516543142906e-07\n",
            "iter: 15000  x: [6.55853549e-12 5.27321084e-08]  f(x): 2.7821231602520676e-15  grad at x: [2.30604040e-07 1.05490451e-07]  gradient norm: 2.5358718135959384e-07\n",
            "iter: 15100  x: [1.14240255e-12 4.46334766e-08]  f(x): 1.9923531510070282e-15  grad at x: [1.81961114e-07 8.92715229e-08]  gradient norm: 2.0268017134928388e-07\n",
            "iter: 15200  x: [-7.79978114e-11  3.77969530e-08]  f(x): 1.4259428226377043e-15  grad at x: [-8.28056224e-08  7.52819147e-08]  gradient norm: 1.1191129427452807e-07\n",
            "iter: 15300  x: [-6.44386069e-11  3.19920785e-08]  f(x): 1.02147548611101e-15  grad at x: [-6.53475069e-08  6.37264025e-08]  gradient norm: 9.127623474479871e-08\n",
            "iter: 15400  x: [-5.32951122e-11  2.70787193e-08]  f(x): 7.317449398427047e-16  grad at x: [-5.15704594e-08  5.39442582e-08]  gradient norm: 7.462905111142311e-08\n",
            "iter: 15500  x: [-4.41260319e-11  2.29199562e-08]  f(x): 5.241995847285719e-16  grad at x: [-4.06982709e-08  4.56634082e-08]  gradient norm: 6.116777015643068e-08\n",
            "iter: 15600  x: [-5.27502382e-11  1.93620131e-08]  f(x): 3.749760306906063e-16  grad at x: [-8.08026621e-08  3.85130253e-08]  gradient norm: 8.951158200531599e-08\n",
            "iter: 15700  x: [-9.65007496e-12  1.63964199e-08]  f(x): 2.6834936517025292e-16  grad at x: [3.66354548e-08 3.27542395e-08]  gradient norm: 4.9142616472735515e-08\n",
            "iter: 15800  x: [-8.86884442e-12  1.38782479e-08]  f(x): 1.9223141294582783e-16  grad at x: [2.89064583e-08 2.77210204e-08]  gradient norm: 4.005044698131254e-08\n",
            "iter: 15900  x: [-8.05978847e-12  1.17468183e-08]  f(x): 1.3770647271125527e-16  grad at x: [2.28079077e-08 2.34613974e-08]  gradient norm: 3.2720602447035036e-08\n",
            "iter: 16000  x: [-7.25835342e-12  9.94273496e-09]  f(x): 9.864833254645314e-17  grad at x: [1.79958796e-08 1.98564365e-08]  gradient norm: 2.6797943076417448e-08\n",
            "iter: 16100  x: [7.30987948e-13 8.39930838e-09]  f(x): 7.05737419342274e-17  grad at x: [3.57901974e-08 1.68015407e-08]  gradient norm: 3.953770349082432e-08\n",
            "iter: 16200  x: [-6.55536757e-14  7.10933712e-09]  f(x): 5.05408164965098e-17  grad at x: [2.82406874e-08 1.42184120e-08]  gradient norm: 3.161802756490841e-08\n",
            "iter: 16300  x: [-1.23112675e-11  6.02039814e-09]  f(x): 3.617606981402798e-17  grad at x: [-1.28522100e-08  1.19915512e-08]  gradient norm: 1.7577730330060377e-08\n",
            "iter: 16400  x: [-1.01752333e-11  5.09578245e-09]  f(x): 2.591489870159417e-17  grad at x: [-1.01425701e-08  1.01508640e-08]  gradient norm: 1.4349626023272268e-08\n",
            "iter: 16500  x: [-8.41897885e-12  4.31316966e-09]  f(x): 1.8564501424232038e-17  grad at x: [-8.00425789e-09  8.59266341e-09]  gradient norm: 1.1743168604108848e-08\n",
            "iter: 16600  x: [-1.01555976e-11  3.64362144e-09]  f(x): 1.3282668794629282e-17  grad at x: [-1.58923071e-08  7.24662048e-09]  gradient norm: 1.746650890036175e-08\n",
            "iter: 16700  x: [-8.29242385e-12  3.08403248e-09]  f(x): 9.51210634594744e-18  grad at x: [-1.25411416e-08  6.13489526e-09]  gradient norm: 1.3961274054263774e-08\n",
            "iter: 16800  x: [-1.58695068e-12  2.61166478e-09]  f(x): 6.8079922304297944e-18  grad at x: [5.68580709e-09 5.21698177e-09]  gradient norm: 7.716560181319e-09\n",
            "iter: 16900  x: [-1.45199836e-12  2.21056374e-09]  f(x): 4.876915560307796e-18  grad at x: [4.48625989e-09 4.41531949e-09]  gradient norm: 6.294567023608855e-09\n",
            "iter: 17000  x: [-1.31483175e-12  1.87106404e-09]  f(x): 3.49363329338607e-18  grad at x: [3.53976093e-09 3.73686876e-09]  gradient norm: 5.147241553821237e-09\n",
            "iter: 17100  x: [-1.18062909e-12  1.58370492e-09]  f(x): 2.502733028545981e-18  grad at x: [2.79293241e-09 3.16268732e-09]  gradient norm: 4.2193675511509856e-09\n",
            "iter: 17200  x: [6.77502687e-14 1.33786385e-09]  f(x): 1.7902491168650834e-18  grad at x: [5.55470619e-09 2.67599869e-09]  gradient norm: 6.16568972930711e-09\n",
            "iter: 17300  x: [-2.35316115e-12  1.13294276e-09]  f(x): 1.2812013632545081e-18  grad at x: [-2.52771242e-09  2.25647288e-09]  gradient norm: 3.388362425483125e-09\n",
            "iter: 17400  x: [-1.94352159e-12  9.58944860e-10]  f(x): 9.177862381584e-19  grad at x: [-1.99478534e-09  1.91011563e-09]  gradient norm: 2.7618309634764737e-09\n",
            "iter: 17500  x: [-1.60696857e-12  8.11669637e-10]  f(x): 6.574638116627922e-19  grad at x: [-1.57422716e-09  1.61691140e-09]  gradient norm: 2.2566775656454506e-09\n",
            "iter: 17600  x: [-1.33013173e-12  6.87013013e-10]  f(x): 4.70985484250965e-19  grad at x: [-1.24234313e-09  1.36870550e-09]  gradient norm: 1.848451027977686e-09\n",
            "iter: 17700  x: [-1.59601953e-12  5.80365626e-10]  f(x): 3.369400776063088e-19  grad at x: [-2.46659608e-09  1.15434717e-09]  gradient norm: 2.7233460374743643e-09\n",
            "iter: 17800  x: [-2.82505769e-13  4.91473636e-10]  f(x): 2.4111067308286663e-19  grad at x: [1.11837724e-09 9.81817250e-10]  gradient norm: 1.4881978230885699e-09\n",
            "iter: 17900  x: [-2.60512383e-13  4.15992819e-10]  f(x): 1.7271834078295516e-19  grad at x: [8.82434130e-10 8.30943589e-10]  gradient norm: 1.2120879682992252e-09\n",
            "iter: 18000  x: [-2.37384712e-13  3.52104393e-10]  f(x): 1.237276943140524e-19  grad at x: [6.96263439e-10 7.03259248e-10]  gradient norm: 9.896243458517822e-10\n",
            "iter: 18100  x: [-2.14248559e-13  2.98027991e-10]  f(x): 8.86341290180164e-20  grad at x: [5.49366287e-10 5.95198988e-10]  gradient norm: 8.099784896197342e-10\n",
            "iter: 18200  x: [2.85013901e-14 2.51764637e-10]  f(x): 6.34153533229651e-20  grad at x: [1.09256272e-09 5.03643279e-10]  gradient norm: 1.2030585366374664e-09\n",
            "iter: 18300  x: [3.23555931e-15 2.13098459e-10]  f(x): 4.54137268596447e-20  grad at x: [8.62100514e-10 4.26209860e-10]  gradient norm: 9.617027296502834e-10\n",
            "iter: 18400  x: [-3.71385631e-13  1.80458105e-10]  f(x): 3.250394030144485e-20  grad at x: [-3.92324474e-10  3.59430667e-10]  gradient norm: 5.320797843006125e-10\n",
            "iter: 18500  x: [-3.06860994e-13  1.52743261e-10]  f(x): 2.3284265431835647e-20  grad at x: [-3.09609938e-10  3.04259078e-10]  gradient norm: 4.340874338417864e-10\n",
            "iter: 18600  x: [-2.53825116e-13  1.29284876e-10]  f(x): 1.6679956848435373e-20  grad at x: [-2.44335846e-10  2.57554451e-10]  gradient norm: 3.5501309951380376e-10\n",
            "iter: 18700  x: [-2.10180527e-13  1.09429240e-10]  f(x): 1.1949022829015793e-20  grad at x: [-1.92824620e-10  2.18017758e-10]  gradient norm: 2.9105511010224165e-10\n",
            "iter: 18800  x: [-2.50867205e-13  9.24421664e-11]  f(x): 8.547192836725387e-21  grad at x: [-3.82832950e-10  1.83880864e-10]  gradient norm: 4.2470370805613794e-10\n",
            "iter: 18900  x: [-4.65203561e-14  7.82832103e-11]  f(x): 6.116940177995632e-21  grad at x: [1.73571773e-10 1.56380339e-10]  gradient norm: 2.3362784691914796e-10\n",
            "iter: 19000  x: [-4.26961786e-14  6.62604278e-11]  f(x): 4.381862474433322e-21  grad at x: [1.36953175e-10 1.32350071e-10]  gradient norm: 1.904539669765606e-10\n",
            "iter: 19100  x: [-3.87590318e-14  5.60841118e-11]  f(x): 3.1389859250954225e-21  grad at x: [1.08059352e-10 1.12013187e-10]  gradient norm: 1.5563989748710454e-10\n",
            "iter: 19200  x: [-3.48739577e-14  4.74706806e-11]  f(x): 2.2486678464976924e-21  grad at x: [8.52608495e-11 9.48018654e-11]  gradient norm: 1.2750218092806182e-10\n",
            "iter: 19300  x: [3.05364087e-15 4.01017309e-11]  f(x): 1.6086526360223616e-21  grad at x: [1.69567846e-10 8.02156764e-11]  gradient norm: 1.8758413914710632e-10\n",
            "iter: 19400  x: [-7.09993147e-14  3.39593327e-11]  f(x): 1.1511532715700492e-21  grad at x: [-7.71606133e-11  6.76346681e-11]  gradient norm: 1.026070590905134e-10\n",
            "iter: 19500  x: [-5.86226080e-14  2.87438418e-11]  f(x): 8.2462320320551905e-22  grad at x: [-6.08924567e-11  5.72531933e-11]  gradient norm: 8.358121453009128e-11\n",
            "iter: 19600  x: [-4.84572958e-14  2.43293485e-11]  f(x): 5.907236261407082e-22  grad at x: [-4.80544932e-11  4.84648679e-11]  gradient norm: 6.825011162547624e-11\n",
            "iter: 19700  x: [-4.00982502e-14  2.05928352e-11]  f(x): 4.231737180149585e-22  grad at x: [-3.79234101e-11  4.10252773e-11]  gradient norm: 5.586822362262159e-11\n",
            "iter: 19800  x: [-4.82934364e-14  1.73961384e-11]  f(x): 3.0276353669482147e-22  grad at x: [-7.52957558e-11  3.45991030e-11]  gradient norm: 8.286464125823093e-11\n",
            "iter: 19900  x: [-3.94387008e-14  1.47244319e-11]  f(x): 2.1681916119761447e-22  grad at x: [-5.94183749e-11  2.92911090e-11]  gradient norm: 6.624584769590364e-11\n",
            "iter: 20000  x: [-7.64612042e-15  1.24691551e-11]  f(x): 1.5518616186131088e-22  grad at x: [2.69382593e-11 2.49077258e-11]  gradient norm: 3.6688753268262106e-11\n",
            "iter: 20100  x: [-6.98717275e-15  1.05541348e-11]  f(x): 1.111680183715264e-22  grad at x: [2.12550210e-11 2.10803209e-11]  gradient norm: 2.993586226703e-11\n",
            "iter: 20200  x: [-6.32073870e-15  8.93322451e-12]  f(x): 7.963656940297488e-23  grad at x: [1.67706819e-11 1.78411661e-11]  gradient norm: 2.4485975138521695e-11\n",
            "iter: 20300  x: [-5.67089056e-15  7.56125460e-12]  f(x): 5.704929343199592e-23  grad at x: [1.32323467e-11 1.50998256e-11]  gradient norm: 2.007734380358896e-11\n",
            "iter: 20400  x: [2.55737132e-16 6.38750878e-12]  f(x): 4.0806900550014654e-23  grad at x: [2.63172465e-11 1.27760405e-11]  gradient norm: 2.9254481263198096e-11\n",
            "iter: 20500  x: [-1.12041885e-14  5.40913189e-12]  f(x): 2.9204588780295565e-23  grad at x: [-1.19760379e-11  1.07734470e-11]  gradient norm: 1.6108775420394956e-11\n",
            "iter: 20600  x: [-9.25489013e-15  4.57839477e-12]  f(x): 2.0920688017864792e-23  grad at x: [-9.45109129e-12  9.11976998e-12]  gradient norm: 1.313367165486183e-11\n",
            "iter: 20700  x: [-7.65316837e-15  3.87524265e-12]  f(x): 1.4986730561953987e-23  grad at x: [-7.45853451e-12  7.71987263e-12]  gradient norm: 1.0734345370685667e-11\n",
            "iter: 20800  x: [-6.33547626e-15  3.28008097e-12]  f(x): 1.0736015045684997e-23  grad at x: [-5.88610492e-12  6.53482003e-12]  gradient norm: 8.794890789371807e-12\n",
            "iter: 20900  x: [-7.59001170e-15  2.77090277e-12]  f(x): 7.680189832021567e-24  grad at x: [-1.16864240e-11  5.51144549e-12]  gradient norm: 1.2920856699754624e-11\n",
            "iter: 21000  x: [-1.36244196e-15  2.34649602e-12]  f(x): 5.496040075586389e-24  grad at x: [5.29865819e-12 4.68754227e-12]  gradient norm: 7.074519851828064e-12\n",
            "iter: 21100  x: [-1.25455901e-15  1.98611975e-12]  f(x): 3.937065711672367e-24  grad at x: [4.18080197e-12 3.96722126e-12]  gradient norm: 5.7635015073880144e-12\n",
            "iter: 21200  x: [-1.14186739e-15  1.68109029e-12]  f(x): 2.820342024494107e-24  grad at x: [3.29875897e-12 3.35761311e-12]  gradient norm: 4.706949814977443e-12\n",
            "iter: 21300  x: [-1.02961417e-15  1.42290744e-12]  f(x): 2.0203955696805165e-24  grad at x: [2.60278725e-12 2.84169643e-12]  gradient norm: 3.85353604937569e-12\n",
            "iter: 21400  x: [1.22755682e-16 1.20202727e-12]  f(x): 1.4454823820530482e-24  grad at x: [5.17637612e-12 2.40454556e-12]  gradient norm: 5.707600995312039e-12\n",
            "iter: 21500  x: [4.93583282e-18 1.01741914e-12]  f(x): 1.0351618204499821e-24  grad at x: [4.08448404e-12 2.03485801e-12]  gradient norm: 4.563294533868875e-12\n",
            "iter: 21600  x: [-1.76837199e-15  8.61580763e-13]  f(x): 7.409177386068077e-25  grad at x: [-1.85879291e-12  1.71608804e-12]  gradient norm: 2.5298358094321435e-12\n",
            "iter: 21700  x: [-1.46131201e-15  7.29258768e-13]  f(x): 5.3075880105904485e-25  grad at x: [-1.46690097e-12  1.45267229e-12]  gradient norm: 2.0644745665369696e-12\n",
            "iter: 21800  x: [-1.20889169e-15  6.17258847e-13]  f(x): 3.8021581620027115e-25  grad at x: [-1.15763969e-12  1.22968213e-12]  gradient norm: 1.6888599082117562e-12\n",
            "iter: 21900  x: [-1.46141691e-15  5.21439617e-13]  f(x): 2.7205472069607733e-25  grad at x: [-2.29849226e-12  1.03703357e-12]  gradient norm: 2.5216076777495735e-12\n",
            "alpha_start: 1.0 rho: 0.5 iter: 21985  x: [-4.78532202e-16  4.53575301e-13]  f(x): 2.0520584176089396e-25  grad at x: [3.78704597e-13 9.05236473e-13]  gradient norm: 9.812595195142256e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpABILpQxPKD",
        "outputId": "816e97a3-6e59-4614-b391-8e5d1c8c9d73"
      },
      "source": [
        "my_start_x = np.array([1.,4000.])\n",
        "my_tol= 1e-12\n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "x_opt, opt_fval, num_iters = find_minimizer_gdscaling(my_start_x, my_tol, 2, alpha_start, rho, gamma)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 1  x: [  -2.16666667 1999.        ]  f(x): 3985718.0  grad at x: [1496.         3989.33333333]  gradient norm: 4260.609867665009 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 2  x: [  -2.416      1001.66666667]  f(x): 1002411.5884444444  grad at x: [-3241.33333333  1993.66933333]  gradient norm: 3805.3855505653505 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 3  x: [-1.33555556  4.832     ]  f(x): 2673.097569185184  grad at x: [-3987.33866667     4.32177778]  gradient norm: 3987.3410088000082 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 4  x: [-0.00644267  2.67111111]  f(x): 7.1282601844938265  grad at x: [-8.64355556  5.31645156]  gradient norm: 10.147694801507992 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 5  x: [-0.00356148  0.01288533]  f(x): 0.019008693825316864  grad at x: [-10.63290311   0.01152474]  gradient norm: 10.632909356800019 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 6  x: [-1.71804444e-05  7.12296296e-03]  f(x): 5.0689850200844984e-05  grad at x: [-0.02304948  0.0141772 ]  gradient norm: 0.02706051947068864 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 7  x: [-9.49728395e-06  3.43608889e-05]  f(x): 1.3517293386891997e-07  grad at x: [-2.83544083e-02  3.07326420e-05]  gradient norm: 0.028354424951466718 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 8  x: [-4.58145185e-08  1.89945679e-05]  f(x): 3.604611569837866e-10  grad at x: [-6.14652840e-05  3.78058777e-05]  gradient norm: 7.21613852551703e-05 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 9  x: [-2.53260905e-08  9.16290370e-08]  f(x): 9.612297519567644e-13  grad at x: [-7.56117555e-05  8.19537119e-08]  gradient norm: 7.561179987057791e-05 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 10  x: [-1.22172049e-10  5.06521811e-08]  f(x): 2.563279338551371e-15  grad at x: [-1.63907424e-07  1.00815674e-07]  gradient norm: 1.9243036068045115e-07 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 11  x: [-6.75362414e-11  2.44344099e-10]  f(x): 6.835411569470324e-18  grad at x: [-2.01631348e-07  2.18543232e-10]  gradient norm: 2.016314663215411e-07 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 12  x: [-3.25792132e-13  1.35072483e-10]  f(x): 1.8227764185254198e-20  grad at x: [-4.37086464e-10  2.68841797e-10]  gradient norm: 5.131476284812136e-10 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 13  x: [-1.80096644e-13  6.51584263e-13]  f(x): 4.860737116067784e-23  grad at x: [-5.37683594e-10  5.82781952e-13]  gradient norm: 5.376839101907762e-10 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 14  x: [-8.68779018e-16  3.60193288e-13]  f(x): 1.2961965642847427e-25  grad at x: [-1.16556390e-12  7.16911459e-13]  gradient norm: 1.3683936759499296e-12 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 15  x: [-4.80257717e-16  1.73755804e-15]  f(x): 3.4565241714259807e-28  grad at x: [-1.43382292e-12  1.55408520e-15]  gradient norm: 1.4338237605087365e-12 Hessian cond num: 1.1089032980269364\n",
            "[[38.72980074  0.05034165]\n",
            " [ 0.05034165  0.99873206]]\n",
            "1.1089032980269364\n",
            "iter: 16  x: [-2.31674405e-18  9.60515434e-16]  f(x): 9.217397790469283e-31  grad at x: [-3.10817041e-15  1.91176389e-15]  gradient norm: 3.6490498025332316e-15 Hessian cond num: 1.1089032980269364\n",
            "alpha_start: 1.0 rho: 0.5 iter: 16  x: [-2.31674405e-18  9.60515434e-16]  f(x): 9.217397790469283e-31  grad at x: [-3.10817041e-15  1.91176389e-15]  gradient norm: 3.6490498025332316e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\Large{\\text{References:}}$\n",
        "\n",
        "\n",
        "\n",
        "1.   Jorge Nocedal and Stephen J. Wright, $\\textit{Numerical Optimization}$. Springer series in Operations Research,  Second Edition, 2006.\n",
        "2.   Amir Beck, $\\textit{Introduction to Nonlinear Optimization}$. MOS-SIAM series on Optimization, 2014.\n",
        "3. Dimitri P. Bertsekas. $\\textit{Nonlinear Programming}$. Athena Scientific, Second Edition, 1999.\n"
      ],
      "metadata": {
        "id": "g_KkUnTn44lz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA1zWRO0xMd6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}